## ğŸš€ ETL with PySpark (Simulation)

This project demonstrates a **simplified ETL** pipeline simulating an AWS job using **PySpark**.  
It processes a small public movie dataset, applies transformations, and writes the results in **Parquet** format.

---

## ğŸ“Œ ETL Steps
1. **Extract** â†’ Load the raw dataset in CSV format (`movies.csv`).  
2. **Transform** â†’ Filter movies released after 2000 and enrich the data with a rating category.  
3. **Load** â†’ Save the transformed output in Parquet format (simulating S3/Glue Data Catalog).

---

## ğŸ—‚ Estrutura do Projeto

```sh
etl-pyspark-simulation/
â”œâ”€â”€ README.md
â”œâ”€â”€ data
â”‚   â””â”€â”€ source
â”œâ”€â”€ etl_job.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ tests
    â”œâ”€â”€ __pycache__/
    â””â”€â”€ test_etl.py
```
### Getting Started

### Prerequisites

This project requires the following dependencies:

- **Programming Language:** Python
- **Package Manager:** Pip

### Installation

Build etl-pyspark-simulation from the source and install dependencies:

1. **Clone the repository:**
   ```sh
   â¯ git clone ../etl-pyspark-simulation
    ```

2. **Navigate to the project directory:**

    ```sh
    â¯ cd etl-pyspark-simulation
    ```

3. **Install the dependencies:**

    ```sh
    â¯ pip install -r requirements.txt
    ```

### Usage

Run the project with:

**Using:**
```sh
python {entrypoint}
```

### Testing:

Etl-pyspark-simulation uses the {__test_framework__} test framework. Run the test suite with:

**Using:**

```sh
pytest
```
